{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "# was running numpy version 1.19.2 which isn't compatible w/ tensorflow\n",
    "# force to run updated version of numpy\n",
    "pkg_resources.require(\"numpy==1.22.2\")  \n",
    "import numpy as np\n",
    "\n",
    "# libraries for data cleaning and preparation for model\n",
    "import os\n",
    "import os.path\n",
    "import chess\n",
    "import chess.pgn\n",
    "\n",
    "# libraries for GAN model\n",
    "\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D,Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation,Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine individual PGN files into larger PGN file containing all player's games, with delimiter of '---'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of all pgn files for one player \n",
    "inputs = []\n",
    "data = \"/Users/zoepratt/Documents/GitHub/Top-Chess-Players/data/Raw_game/Raw_game/Alekhine\"\n",
    "for file in os.listdir(data):\n",
    "    if file.endswith(\".pgn\"):\n",
    "        inputs.append(os.path.join(data, file))\n",
    " \n",
    " # concatanate all pgn files in a file called names for the player\n",
    "with open('merged_file.pgn', 'w') as outfile:\n",
    "    for fname in inputs:\n",
    "        with open(fname, encoding=\"utf-8\", errors = 'ignore') as infile:\n",
    "            outfile.write(infile.read())\n",
    "            outfile.write('---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare PGN files for input into the GAN\n",
    "\n",
    "- Extract all game moves from the PGN file\n",
    "- Record gameplay moves of the target player \n",
    " - e.g., moves of white pieces when target player is playing white \n",
    "- Convert game board layout into a matrix\n",
    "- Convert larger matrix into a matrix of one-hot encoded values\n",
    "- Transform matrix into a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractdata(pgn):\n",
    "    '''\n",
    "    input: pgn files of player\n",
    "    output: \n",
    "        function returns step-by-step gameplay as a list\n",
    "        function returns which player is playing White as a list\n",
    "    \n",
    "    list 'side' will be used to ensure only moves made by \n",
    "    intended player will be used in creation of the GAN\n",
    "    '''\n",
    "    \n",
    "    side = []\n",
    "    game_moves = []\n",
    "    length = 1 #used for training purposes, to remove for DA servers\n",
    "    for index in range(length):\n",
    "        try:\n",
    "            if chess.pgn.read_game(pgn).mainline_moves():\n",
    "                # extracts game moves from the pgn files\n",
    "                game_moves.append(chess.pgn.read_game(pgn).mainline_moves()) \n",
    "                \n",
    "                # extracts player's name playing white from pgn files\n",
    "                side.append(chess.pgn.read_game(pgn).headers[\"White\"]) \n",
    "                \n",
    "        except:\n",
    "            print(index,chess.pgn.read_game(pgn))\n",
    "            pass\n",
    "\n",
    "    return game_moves, side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_moves(game_moves, side, name):\n",
    "    '''\n",
    "    input: game_moves and side list from extractdata function\n",
    "    output:\n",
    "        function returns 2 lists, which contain all of player's move\n",
    "        list PW: player's moves when they are playing white\n",
    "        list PB: player's moves when they are playing black \n",
    "    '''\n",
    "    \n",
    "    PW = [] # empty list for all moves when player playing white\n",
    "    PB = [] # empty list for all moves when player playing black\n",
    "    \n",
    "    match = 0\n",
    "    \n",
    "    for game in game_moves:\n",
    "        board = chess.Board() # saves FEN notation of chess board\n",
    "        white = side[match]\n",
    "        if white == name:\n",
    "            identifier = 0\n",
    "        else:\n",
    "            identifier = 1\n",
    "        \n",
    "        play = 0\n",
    "        for move in game:\n",
    "            if play % 2 == identifier: # creates list PW of moves when the player is playing white\n",
    "                PW.append(board.copy())\n",
    "            board.push(move) # move game forward one move\n",
    "            if play % 2 == identifier: # creates list PB of moves when the player is playing black\n",
    "                PB.append(board.copy())\n",
    "            play = play + 1\n",
    "        match = match + 1\n",
    "    \n",
    "    return PW, PB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_matrix(board): \n",
    "    '''\n",
    "    input: FEN notation of a board position\n",
    "    output: matrix representing board position at a given moment\n",
    "        each board square is an individual item in the matrix, blank squares are represented by a period\n",
    "    '''\n",
    "    \n",
    "    pgn = board.epd() # convert FEN notation of board into EPD notation\n",
    "    matrix = []  \n",
    "\n",
    "    # retrieve only the first field from EPD notation: the piece placement\n",
    "    pieces = pgn.split(\" \", 1)[0] \n",
    "    \n",
    "    # separate into placement of individual pieces\n",
    "    rows = pieces.split(\"/\")\n",
    "    \n",
    "    # separates rows so that each specific square on the board is its own list entry, formatted as a matrix\n",
    "    for row in rows:\n",
    "        game_row = []  \n",
    "        for item in row:\n",
    "            if item.isdigit():\n",
    "            \n",
    "            # replaces numbers in epd that represent the number of empty squares with a period for each empty square\n",
    "            # example: '8' = '........'\n",
    "            \n",
    "                for i in range(0, int(item)):\n",
    "                    game_row.append('.')\n",
    "            else:\n",
    "                game_row.append(item)\n",
    "        matrix.append(game_row)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate game pieces to binary values using one-hot encoding\n",
    "# each game piece is represented by a unique binary vector\n",
    "\n",
    "chess_dict = {\n",
    "    'p' : [1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    'P' : [0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "    'n' : [0,1,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    'N' : [0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "    'b' : [0,0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "    'B' : [0,0,0,0,0,0,0,0,1,0,0,0,0],\n",
    "    'r' : [0,0,0,1,0,0,0,0,0,0,0,0,0],\n",
    "    'R' : [0,0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "    'q' : [0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "    'Q' : [0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
    "    'k' : [0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
    "    'K' : [0,0,0,0,0,0,0,0,0,0,0,1,0],\n",
    "    '.' : [0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(matrix, chess_dict):\n",
    "    '''\n",
    "    input: matrix created in the previous function, chess dictionary\n",
    "    output: layout of a chess board represented by a matrix of one-hot encoded values\n",
    "    '''\n",
    "    \n",
    "    rows = []\n",
    "    for row in matrix:\n",
    "        pieces = []\n",
    "        for piece in row:\n",
    "\n",
    "            # appends one-hot endoded value associated with each chess piece, pulled from chess_dict\n",
    "            pieces.append(chess_dict[piece])\n",
    "        rows.append(pieces)\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_matrix(X, Y):\n",
    "    '''\n",
    "    inputs:\n",
    "        X: list of moves when player is playing white\n",
    "        Y: list of moves when player is playing black\n",
    "        \n",
    "    translate and make_matrix functions to convert each instance of the game board into a one-hot encoded matrix\n",
    "    then transforms matrix into numpy array\n",
    "    \n",
    "    outputs:\n",
    "        X_array: numpy array of all moves when player is playing white\n",
    "        Y_array: numpy array of all moves when player is playing black\n",
    "    '''\n",
    "    for i in range(len(X)):\n",
    "        X[i] = translate(make_matrix(X[i]),chess_dict)\n",
    "    for i in range(len(Y)):\n",
    "        Y[i] = translate(make_matrix(Y[i]),chess_dict)\n",
    "    X_array = np.array(X)\n",
    "    Y_array = np.array(Y)\n",
    "\n",
    "    return X_array, Y_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator():\n",
    "    '''\n",
    "    create the PatchGAN discriminator model\n",
    "    \n",
    "    model takes a chess move from the source domain (real move) and a chess move from the target domain \n",
    "    (imitated move) and predicts the likelihood (known as the error) of whether the move from the target \n",
    "    domain is a real or generated version of the chess player's gameplay\n",
    "    '''\n",
    "    \n",
    "    # initiatlize as randomly distributed with a SD of 0.02\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    # real chess move input\n",
    "    in_src_image = Input(shape=image_shape)\n",
    "    \n",
    "    # imitated chess move input\n",
    "    in_target_image = Input(shape=image_shape)\n",
    "\n",
    "    # concatenate images channel-wise\n",
    "    # create one 256x256x6 input to the first hidden convolutional layer\n",
    "    merged = concatenate([in_src_image, in_target_image])\n",
    "    \n",
    "    # C64\n",
    "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    # C128\n",
    "    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    \n",
    "    #normalizes output using moving average of mean and standard deviation of the batches it has seen during training.\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    # C256\n",
    "    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    # C512\n",
    "    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    # second to last output layer\n",
    "    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "\n",
    "    # patch output\n",
    "    d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    patch_out = Activation('sigmoid')(d)\n",
    "    \n",
    "    # define model\n",
    "    model = Model(inputs = [in_src_image, in_target_image], outputs = patch_out)\n",
    "\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "    '''\n",
    "    this function defines the encoder block of the final generator function\n",
    "    \n",
    "    inputs: \n",
    "        current layer of model (layer_in), number of filters (n_filters), presence or absence of batch normalization\n",
    "        \n",
    "    output: encoded layer\n",
    "    '''\n",
    "    # initiatlize as randomly distributed with a SD of 0.02\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "   \n",
    "    # add downsampling layer\n",
    "    # flattes given image to a 2D structure\n",
    "    # convolutional layers that use a 2Ã—2 stride to downsample the input source image down to a bottleneck layer\n",
    "    g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    \n",
    "    # conditionally add batch normalization\n",
    "    # normalizes its output using the mean and standard deviation of the current batch of inputs\n",
    "    if batchnorm:\n",
    "        g = BatchNormalization()(g, training=True)\n",
    "    \n",
    "    # leaky relu activation\n",
    "    # more balanced than regular relu, may learn faster\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "    '''\n",
    "    this function defines the decoder block of the final generator function\n",
    "    uses transpose convolutional layers to upsample to required output image size\n",
    "    \n",
    "    inputs: \n",
    "        current layer of model, encoded (layer_in), \n",
    "        skip connection input (skip_in), number of filters (n_filters), \n",
    "        introduction of dropout layers as a source of randomness\n",
    "    \n",
    "    output: decoded layer\n",
    "    '''\n",
    "\n",
    "    # initiatlize as randomly distributed with a SD of 0.02\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    \n",
    "    # add unsampling layer\n",
    "    g = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    \n",
    "    # add batch normalization\n",
    "    # normalizes its output using the mean and standard deviation of the current batch of inputs\n",
    "    g = BatchNormalization()(g, training=True)\n",
    "    \n",
    "    # conditionally add dropout\n",
    "    if dropout:\n",
    "        g = Dropout(0.5)(g, training=True)\n",
    "    \n",
    "    # merge with skip connection\n",
    "    g = concatenate([g, skip_in])\n",
    "    # relu activation\n",
    "    g = Activation('relu')(g)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(image_shape=(8,8,13)):\n",
    "    '''\n",
    "    function defines standalone generator model\n",
    "\n",
    "    inputs: image_shape, for chess board it is 8 x 8 x 13\n",
    "        8 x 8 represents size of the board, 13 represents depth, or number of possible pieces that could\n",
    "        fill each chess board square, as defined above in chess_dict\n",
    "        \n",
    "    output: standalone generator model that creates chess gameplay moves that attempt to imitate the player\n",
    "    and 'trick' the discriminator into believing it is a real move by that player\n",
    "    '''\n",
    "\n",
    "    # initiatlize as randomly distributed with a SD of 0.02\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # image input\n",
    "    in_image = Input(shape=image_shape)\n",
    "\n",
    "    # encoder model\n",
    "    e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "    e2 = define_encoder_block(e1, 128)\n",
    "    b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e2)\n",
    "    b = Activation('relu')(b)\n",
    "    \n",
    "    # decoder model\n",
    "    d6 = define_decoder_block(b, e2, 128, dropout=False)\n",
    "    d7 = define_decoder_block(d6, e1, 64, dropout=False)\n",
    "    g = Conv2DTranspose(13, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
    "\n",
    "    # 'softmax' converts a vector of values to a probability distribution\n",
    "    out_image = Activation('softmax')(g)\n",
    "\n",
    "    # define generator model itself\n",
    "    model = Model(in_image, out_image)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model, image_shape):\n",
    "    '''\n",
    "    function defined the final GAN model\n",
    "    inputs: generator model, discriminator model, shape of image to be generated \n",
    "        shape will be 8x8x13 to represent possible positions on a chess board\n",
    "        \n",
    "    output: usable GAN model\n",
    "    '''\n",
    "    d_model.trainable = False\n",
    "    # define source image\n",
    "    in_src = Input(shape=image_shape)\n",
    "    # connect source input to the generator input\n",
    "    gen_out = g_model(in_src)\n",
    "    # connect source input and generator output to the discriminator input\n",
    "    dis_out = d_model([in_src, gen_out])\n",
    "    # source image as input, generated image and classification as output\n",
    "    model = Model(in_src, [dis_out, gen_out])\n",
    "\n",
    "    # compile final model with Adam optimizer\n",
    "    # Adam optimizer:\n",
    "        # computationally efficient, doesn't require a lot of memory, maintains single learning rate\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "    '''\n",
    "    function pulls real samples of chess moves from the dataset\n",
    "    \n",
    "    inputs: pgn file of players moves (dataset), number of samples to create (n_samples),\n",
    "    shape of discriminator model output (patch_shape)\n",
    "    \n",
    "    outputs: \n",
    "    '''\n",
    "    trainA, trainB = dataset\n",
    "    # choose random chess moves from dataset\n",
    "    ix = randint(0, trainA.shape[0], n_samples)\n",
    "    \n",
    "    X1, X2 = trainA[ix], trainB[ix]\n",
    "    y = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "    return [X1, X2], y\n",
    " \n",
    "def generate_fake_samples(g_model, samples, patch_shape):\n",
    "    '''\n",
    "    function creates fake samples of chess moves using the generator model\n",
    "    \n",
    "    inputs: generator model (g_model), real samples from dataset (samples), \n",
    "    and shape of discriminator model output (patch_shape)\n",
    "    \n",
    "    outputs:\n",
    "    '''\n",
    "    X = g_model.predict(samples)\n",
    "    y = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=1):\n",
    "    '''\n",
    "    function trains the model to imitate the chess player\n",
    "    \n",
    "    inputs: disrimiantor model, generator model, combined GAN model, data to be used\n",
    "    '''\n",
    "    n_patch = d_model.output_shape[1]\n",
    "    trainA, trainB = dataset\n",
    "    bat_per_epo = int(len(trainA) / n_batch)\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    for i in range(n_steps):\n",
    "        [X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
    "        X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
    "        d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
    "        d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
    "        g_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
    "       # print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "    if (i+1) % (bat_per_epo * 10) == 0:\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-20092728fe94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mgan_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_array\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmain_adams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-88-20092728fe94>\u001b[0m in \u001b[0;36mmain_adams\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mgan_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_array\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mmain_adams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-5b227c7d50a1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(d_model, g_model, gan_model, dataset, n_epochs, n_batch)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mX_fakeB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_fake_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_patch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0md_loss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0md_loss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_fakeB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m        \u001b[0;31m# print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2087\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2088\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2089\u001b[0;31m       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n\u001b[0m\u001b[1;32m   2090\u001b[0m                                                     \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2091\u001b[0m                                                     class_weight)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36msingle_batch_iterator\u001b[0;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1639\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1640\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_distribute_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1641\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 755\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    785\u001b[0m                 \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m                 output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m         \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m         self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3313\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3314\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3315\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3316\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3317\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "adams_pgn = open(\"/Users/zoepratt/Documents/GitHub/Top-Chess-Players/data/test_Adams.pgn\")\n",
    "\n",
    "def main_adams():\n",
    "    game_moves, side = extractdata(adams_pgn)\n",
    "    PW, PB = categorize_moves(game_moves, side, 'Adams, Michael')\n",
    "    X_array, Y_array = one_hot_matrix(PW, PB)\n",
    "    image_shape = (8,8,13)\n",
    "    d_model = define_discriminator()\n",
    "    g_model = define_generator()\n",
    "    gan_model = define_gan(g_model, d_model, image_shape)\n",
    "    train(d_model, g_model, gan_model, [X_array, Y_array])\n",
    "main_adams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

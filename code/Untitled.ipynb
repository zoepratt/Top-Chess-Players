{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pkg_resources\n",
    "# was running numpy version 1.19.2 which isn't compatible w/ tensorflow\n",
    "# force to run updated version of numpy\n",
    "pkg_resources.require(\"numpy==1.22.2\")  \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chess.pgn\n",
    "pgn = open(\"/Users/zoepratt/Documents/GitHub/Top-Chess-Players/data/test_Adams.pgn\")\n",
    "sides = []\n",
    "games = []\n",
    "length = 3\n",
    "for i in range(length):\n",
    "    try:\n",
    "        if chess.pgn.read_game(pgn).mainline_moves():\n",
    "            games.append(chess.pgn.read_game(pgn).mainline_moves())\n",
    "            sides.append(chess.pgn.read_game(pgn).headers[\"White\"])\n",
    "    except:\n",
    "        print(i,chess.pgn.read_game(pgn))\n",
    "        pass\n",
    "len(games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "counter2 = 0\n",
    "for game in games:\n",
    "    board = chess.Board()\n",
    "    white = sides[counter2]\n",
    "    if white == 'Carlsen,Magnus':\n",
    "        remainder = 0\n",
    "    else:\n",
    "        remainder = 1\n",
    "    counter = 0\n",
    "    for move in game:\n",
    "        if counter % 2 == remainder:\n",
    "            X.append(board.copy())\n",
    "        board.push(move)\n",
    "        if counter % 2 == remainder:\n",
    "            y.append(board.copy())\n",
    "        counter += 1\n",
    "    counter2 += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "chess_dict = {\n",
    "    'p' : [1,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    'P' : [0,0,0,0,0,0,1,0,0,0,0,0,0],\n",
    "    'n' : [0,1,0,0,0,0,0,0,0,0,0,0,0],\n",
    "    'N' : [0,0,0,0,0,0,0,1,0,0,0,0,0],\n",
    "    'b' : [0,0,1,0,0,0,0,0,0,0,0,0,0],\n",
    "    'B' : [0,0,0,0,0,0,0,0,1,0,0,0,0],\n",
    "    'r' : [0,0,0,1,0,0,0,0,0,0,0,0,0],\n",
    "    'R' : [0,0,0,0,0,0,0,0,0,1,0,0,0],\n",
    "    'q' : [0,0,0,0,1,0,0,0,0,0,0,0,0],\n",
    "    'Q' : [0,0,0,0,0,0,0,0,0,0,1,0,0],\n",
    "    'k' : [0,0,0,0,0,1,0,0,0,0,0,0,0],\n",
    "    'K' : [0,0,0,0,0,0,0,0,0,0,0,1,0],\n",
    "    '.' : [0,0,0,0,0,0,0,0,0,0,0,0,1],\n",
    "}\n",
    "def make_matrix(board): \n",
    "    pgn = board.epd()\n",
    "    foo = []  \n",
    "    pieces = pgn.split(\" \", 1)[0]\n",
    "    rows = pieces.split(\"/\")\n",
    "    for row in rows:\n",
    "        foo2 = []  \n",
    "        for thing in row:\n",
    "            if thing.isdigit():\n",
    "                for i in range(0, int(thing)):\n",
    "                    foo2.append('.')\n",
    "            else:\n",
    "                foo2.append(thing)\n",
    "        foo.append(foo2)\n",
    "    return foo\n",
    "def translate(matrix,chess_dict):\n",
    "    rows = []\n",
    "    for row in matrix:\n",
    "        terms = []\n",
    "        for term in row:\n",
    "            terms.append(chess_dict[term])\n",
    "        rows.append(terms)\n",
    "    return rows\n",
    "import numpy as np\n",
    "for i in range(len(X)):\n",
    "    X[i] = translate(make_matrix(X[i]),chess_dict)\n",
    "for i in range(len(y)):\n",
    "    y[i] = translate(make_matrix(y[i]),chess_dict)\n",
    "X = np.array(X)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from numpy import expand_dims\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D,Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation,Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator():\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    in_src_image = Input(shape=image_shape)\n",
    "    in_target_image = Input(shape=image_shape)\n",
    "    merged = concatenate([in_src_image, in_target_image])\n",
    "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    patch_out = Activation('sigmoid')(d)\n",
    "    model = Model(inputs = [in_src_image, in_target_image], outputs = patch_out)\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    if batchnorm:\n",
    "        g = BatchNormalization()(g, training=True)\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    return g\n",
    " \n",
    "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    g = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    g = BatchNormalization()(g, training=True)\n",
    "    if dropout:\n",
    "        g = Dropout(0.5)(g, training=True)\n",
    "    g = concatenate([g, skip_in])\n",
    "    g = Activation('relu')(g)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(image_shape=(8,8,13)):\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    in_image = Input(shape=image_shape)\n",
    "    e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "    e2 = define_encoder_block(e1, 128)\n",
    "    b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e2)\n",
    "    b = Activation('relu')(b)\n",
    "    d6 = decoder_block(b, e2, 128, dropout=False)\n",
    "    d7 = decoder_block(d6, e1, 64, dropout=False)\n",
    "    g = Conv2DTranspose(13, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
    "    out_image = Activation('softmax')(g)\n",
    "    model = Model(in_image, out_image)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(g_model, d_model, image_shape):\n",
    "    d_model.trainable = False\n",
    "    in_src = Input(shape=image_shape)\n",
    "    gen_out = g_model(in_src)\n",
    "    dis_out = d_model([in_src, gen_out])\n",
    "    model = Model(in_src, [dis_out, gen_out])\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "    trainA, trainB = dataset\n",
    "    ix = randint(0, trainA.shape[0], n_samples)\n",
    "    X1, X2 = trainA[ix], trainB[ix]\n",
    "    y = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "    return [X1, X2], y\n",
    " \n",
    "def generate_fake_samples(g_model, samples, patch_shape):\n",
    "    X = g_model.predict(samples)\n",
    "    y = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(d_model, g_model, gan_model, dataset, n_epochs=1, n_batch=1):\n",
    "    n_patch = d_model.output_shape[1]\n",
    "    trainA, trainB = dataset\n",
    "    bat_per_epo = int(len(trainA) / n_batch)\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    print(bat_per_epo)\n",
    "    print(n_steps)\n",
    "    for i in range(n_steps):\n",
    "        [X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
    "        X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
    "        d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
    "        d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
    "        g_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
    "        print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "    if (i+1) % (bat_per_epo * 10) == 0:\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n",
      "145\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable float object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-19ad3a0b8dde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgan_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_gan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-180-759126078fc9>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(d_model, g_model, gan_model, dataset, n_epochs, n_batch)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mX_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_real_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_patch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mX_fakeB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_fake_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_patch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0md_loss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0md_loss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_fakeB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_realA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0my_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_realB\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable float object"
     ]
    }
   ],
   "source": [
    "image_shape = (8,8,13)\n",
    "d_model = define_discriminator()\n",
    "g_model = define_generator()\n",
    "gan_model = define_gan(g_model, d_model, image_shape)\n",
    "train(d_model, g_model, gan_model, [X,y])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "instance = random.randint(1,len(X)-1)\n",
    "state = X[instance].reshape(1,8,8,13)\n",
    "action = gan_model.predict(state)[1]\n",
    "def retranslate(action):\n",
    "    board = []\n",
    "    flatten_action = flatten(flatten(action))\n",
    "    for i in range(len(flatten_action)):\n",
    "        new_set = np.zeros((13,))\n",
    "        max_index = list(flatten_action[i]).index(max(flatten_action[i]))\n",
    "        new_set[max_index] = 1\n",
    "        board.append(new_set)\n",
    "    for i in range(len(board)):\n",
    "        print(board[i])\n",
    "        board[i] = new_chess_dict[tuple(board[i])]\n",
    "    board = np.array(board).reshape(8,8)\n",
    "    print(board)\n",
    "        \n",
    "retranslate(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "instance = random.randint(1,len(X)-1)\n",
    "state = X[instance].reshape(1,8,8,13)\n",
    "action = gan_model.predict(state)[1]\n",
    "\n",
    "board = []\n",
    "flatten_action = flatten(flatten(action))\n",
    "for i in range(len(flatten_action)):\n",
    "    new_set = np.zeros((13,))\n",
    "    max_index = list(flatten_action[i]).index(max(flatten_action[i]))\n",
    "    new_set[max_index] = 1\n",
    "    board.append(new_set)\n",
    "for i in range(len(board)):\n",
    "    print(board[i])\n",
    "    board[i] = new_chess_dict[tuple(board[i])]\n",
    "board = np.array(board).reshape(8,8)\n",
    "print(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
